{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.contrib import itertools\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DQL_MODEL = {\n",
    "    \"join\": [\"inner_join\", \"outer_join\", \"self_join\"],\n",
    "    \"nesting\": [\"cte\", \"correlated_subquery\", \"uncorrelated_subquery\"],\n",
    "    \"predicates\": [\"basic_operators\", \"logical_operators\", \"set_operators\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgbeta(n: int, mean: float, var: float, min: float = 0, max: float = 1) -> float:\n",
    "    dmin = mean - min\n",
    "    dmax = max - mean\n",
    "\n",
    "    if dmin <= 0 or dmax <= 0:\n",
    "        raise ValueError(f\"mean must be between min = {min} and max = {max}\")\n",
    "\n",
    "    if var >= dmin * dmax:\n",
    "        raise ValueError(\n",
    "            f\"var must be less than (mean - min) * (max - mean) = {dmin * dmax}\")\n",
    "\n",
    "    mx = (mean - min) / (max - min)\n",
    "    vx = var / (max - min) ** 2\n",
    "\n",
    "    a = ((1 - mx) / vx - 1 / mx) * mx ** 2\n",
    "    b = a * (1 / mx - 1)\n",
    "\n",
    "    x = np.random.beta(a, b, n)\n",
    "    y = (max - min) * x + min\n",
    "\n",
    "    return y.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_complexity(frequency, regulation: float, weight=1):\n",
    "    return ((frequency*weight)**(1/regulation))/(1+((frequency*weight)**(1/regulation))) \n",
    "\n",
    "def calc_frequency(complexity, regulation: float, weight=1):\n",
    "    x = -complexity/(1-complexity)\n",
    "    if(x<0): x = x*-1\n",
    "    return x**regulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_random_task(dql_model: dict[str, list[str]]):\n",
    "    return {key: [random.randint(0, 7) for _ in dql_model[key]] for key in dql_model}\n",
    "\n",
    "\n",
    "def create_optimal_task(dql_model: dict[str, list[str]], learner_competency: dict[str, list[float]], scaffolding_bonus: dict[str, list[float]], regulation: float):\n",
    "    return {key: [np.clip(calc_frequency(learner_competency[key][i] + scaffolding_bonus[key][i], regulation), 0, 7) for i in range(len(dql_model[key]))] for key in dql_model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_task_complexities(task: dict[str, list[int]], regulation: float):\n",
    "    return {key: calc_complexity_for_category(category, regulation) for key, category in task.items()}\n",
    "\n",
    "\n",
    "def calc_complexity_for_category(category: list[int], regulation: float):\n",
    "    return list(calc_complexity(frequency, regulation) for frequency in category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_learner_scaffolded_competence_bonuses(dql_model: dict[str, list[str]], bonusDistribution: tuple[4]):\n",
    "    return {key: rgbeta(len(dql_model[key]), *bonusDistribution) for key in dql_model}\n",
    "\n",
    "\n",
    "def sample_from_snd_vectorized_and_normalize(X: list[float], mean=0, sd=1):\n",
    "    # Generate random normal samples and normalize using min-max\n",
    "    \n",
    "    samples = np.random.normal(mean, sd, len(X))\n",
    "    min_x = min(samples)\n",
    "    max_x = max(samples)\n",
    "    normalized = (samples - min_x) / (max_x - min_x)\n",
    "    return (normalized-0.2).tolist()\n",
    "\n",
    "\n",
    "def create_learner_competencies(dql_model: dict[str, list[str]]):\n",
    "    return {key: sample_from_snd_vectorized_and_normalize(dql_model[key]) for key in dql_model}\n",
    "\n",
    "\n",
    "def create_learner_population(learner_count: int, task_count: int, dql_model: dict[str, list[str]], bonusDistribution: tuple[4]):\n",
    "    population = {\n",
    "        \"learner_competencies\": [create_learner_competencies(dql_model) for _ in range(learner_count)],\n",
    "        \"scaffolding_competence_bonus_per_step_and_learner\": [[create_learner_scaffolded_competence_bonuses(dql_model, bonusDistribution) for _ in range(learner_count)] for _ in range(task_count)]\n",
    "    }\n",
    "    # Calculate and print mean competency stats for better insight\n",
    "    mean_competencies = {}\n",
    "    for key in dql_model:\n",
    "        mean_competencies[key] = sum([sum(population[\"learner_competencies\"][i][key]) / len(population[\"learner_competencies\"][i][key]) \n",
    "                                    for i in range(learner_count)]) / learner_count\n",
    "    \n",
    "    print(f\"Mean initial learner competencies:\")\n",
    "    for key, value in mean_competencies.items():\n",
    "        print(f\"  {key}: {value:.4f}\")\n",
    "    return population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def calculate_delta(learner_competency: dict[str, list[str]], task_complexities: dict[str, list[str]], scaffolding_bonus: dict[str, list[str]]):\n",
    "    result = {}\n",
    "    for key in learner_competency:\n",
    "        result[key] = []\n",
    "        for i in range(len(learner_competency[key])):\n",
    "            k = learner_competency[key][i]\n",
    "            c = task_complexities[key][i]\n",
    "            t = scaffolding_bonus[key][i]\n",
    "            if (c <= k or c > k + t):\n",
    "                result[key].append(0)\n",
    "            else:\n",
    "                result[key].append(c - k)\n",
    "    return result\n",
    "\n",
    "\n",
    "def add_delta_to_competency(competency: dict[str, list[str]], delta: dict[str, list[str]]):\n",
    "    return {key: [competency[key][i] + delta[key][i] for i in range(len(competency[key]))] for key in competency}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_task_adaptation(task_count: int, learner_count: int, regulation: float, bonusDistribution: tuple[4]):\n",
    "\n",
    "    learner_population = create_learner_population(\n",
    "        learner_count, task_count, DQL_MODEL, bonusDistribution)\n",
    "\n",
    "    simulationLog = [{\n",
    "        \"tasks\": [],\n",
    "        \"competencies\": [],\n",
    "        \"scaffolding_bonuses\": [],\n",
    "        \"deltas\": []\n",
    "    } for _ in range(learner_count)]\n",
    "\n",
    "    for i, j in itertools.product(range(task_count), range(learner_count)):\n",
    "        learner_competency = learner_population[\"learner_competencies\"][j]\n",
    "        scaffolding_bonus = learner_population[\"scaffolding_competence_bonus_per_step_and_learner\"][i][j]\n",
    "\n",
    "        # task = create_random_task(dql_model)\n",
    "        task = create_optimal_task(\n",
    "            DQL_MODEL, learner_competency, scaffolding_bonus, regulation)\n",
    "\n",
    "        task_complexities = calc_task_complexities(task, regulation)\n",
    "        delta = calculate_delta(\n",
    "            learner_competency, task_complexities, scaffolding_bonus)\n",
    "\n",
    "        # update the learner competency in the global learner population\n",
    "        learner_population[\"learner_competencies\"][j] = add_delta_to_competency(\n",
    "            learner_competency, delta)\n",
    "\n",
    "        simulationLog[j][\"tasks\"].append(task)\n",
    "        simulationLog[j][\"competencies\"].append(learner_competency)\n",
    "        simulationLog[j][\"scaffolding_bonuses\"].append(scaffolding_bonus)\n",
    "        simulationLog[j][\"deltas\"].append(delta)\n",
    "\n",
    "    return simulationLog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_simulation_log(simulationLog: dict[str, list[list[float]]], learnerId: int, regulation: float):\n",
    "    task_count = len(simulationLog[learnerId][\"tasks\"])\n",
    "    plt.figure(figsize=(16, 9))\n",
    "    colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n",
    "    for i, key in enumerate(DQL_MODEL):\n",
    "        color = colors[i % len(colors)]\n",
    "\n",
    "        aggregated_competency_values = []\n",
    "        aggregated_task_values = []\n",
    "        aggregated_competency_plus_bonus_values = []\n",
    "        for i in range(task_count):\n",
    "            # Get competency value\n",
    "            competency_values = simulationLog[learnerId][\"competencies\"][i][key]\n",
    "            competency_aggregated = sum(\n",
    "                competency_values)/len(competency_values)\n",
    "            aggregated_competency_values.append(competency_aggregated)\n",
    "\n",
    "            # Get task complexity value\n",
    "            task_values = calc_task_complexities(\n",
    "                simulationLog[learnerId][\"tasks\"][i], regulation)[key]\n",
    "            task_aggregated = sum(task_values)/len(task_values)\n",
    "            aggregated_task_values.append(task_aggregated)\n",
    "\n",
    "            scaffolding_bonus_values = simulationLog[learnerId][\"scaffolding_bonuses\"][i][key]\n",
    "            # Add the scaffolding bonus to the competency value\n",
    "            competency_plus_bonus_values = [\n",
    "                a + b for a, b in zip(scaffolding_bonus_values, competency_values)]\n",
    "            competency_plus_bonus_aggregated = sum(\n",
    "                competency_plus_bonus_values)/len(competency_plus_bonus_values)\n",
    "            aggregated_competency_plus_bonus_values.append(\n",
    "                competency_plus_bonus_aggregated)\n",
    "\n",
    "        plt.plot(range(task_count), aggregated_competency_values,\n",
    "                 color=color, label=f'{key} competency')\n",
    "        plt.plot(range(task_count), aggregated_task_values,\n",
    "                 '.', color=color, label=f'{key} task')\n",
    "        plt.plot(range(task_count), aggregated_competency_plus_bonus_values,\n",
    "                 '--', color=color, label=f'{key} competency + scaffolding bonus')\n",
    "\n",
    "    plt.ylim(0, 1)\n",
    "    plt.xlim(0, task_count-1)\n",
    "    plt.ylabel(\"Competency\")\n",
    "    plt.xlabel(\"Step\")\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\n",
    "        f\"sql_task_adaptation_{learnerId}.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    \n",
    "    \n",
    "def plot_mean_simulation_log(simulationLog: dict[str, list[list[float]]], regulation: float):\n",
    "    task_count = len(simulationLog[0][\"tasks\"])\n",
    "    plt.figure(figsize=(16, 9))\n",
    "    colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n",
    "    \n",
    "    for i, key in enumerate(DQL_MODEL):\n",
    "        color = colors[i % len(colors)]\n",
    "\n",
    "        # Initialize arrays to store values across all learners\n",
    "        mean_competency_values = []\n",
    "        mean_task_values = []\n",
    "        mean_competency_plus_bonus_values = []\n",
    "        \n",
    "        # For each task step\n",
    "        for step in range(task_count):\n",
    "            # Collect values across all learners for this step\n",
    "            all_learners_competency = []\n",
    "            all_learners_task = []\n",
    "            all_learners_competency_plus_bonus = []\n",
    "            \n",
    "            # Iterate through all learners\n",
    "            for learner_id in range(len(simulationLog)):\n",
    "                # Get competency value\n",
    "                competency_values = simulationLog[learner_id][\"competencies\"][step][key]\n",
    "                competency_aggregated = sum(competency_values)/len(competency_values)\n",
    "                all_learners_competency.append(competency_aggregated)\n",
    "                \n",
    "                # Get task complexity value\n",
    "                task_values = calc_task_complexities(\n",
    "                    simulationLog[learner_id][\"tasks\"][step],regulation)[key]\n",
    "                task_aggregated = sum(task_values)/len(task_values)\n",
    "                all_learners_task.append(task_aggregated)\n",
    "                \n",
    "                # Get competency + scaffolding bonus\n",
    "                scaffolding_bonus_values = simulationLog[learner_id][\"scaffolding_bonuses\"][step][key]\n",
    "                competency_plus_bonus_values = [\n",
    "                    a + b for a, b in zip(scaffolding_bonus_values, competency_values)]\n",
    "                competency_plus_bonus_aggregated = sum(\n",
    "                    competency_plus_bonus_values)/len(competency_plus_bonus_values)\n",
    "                all_learners_competency_plus_bonus.append(competency_plus_bonus_aggregated)\n",
    "            \n",
    "            # Calculate means across all learners\n",
    "            mean_competency_values.append(sum(all_learners_competency)/len(all_learners_competency))\n",
    "            mean_task_values.append(sum(all_learners_task)/len(all_learners_task))\n",
    "            mean_competency_plus_bonus_values.append(sum(all_learners_competency_plus_bonus)/len(all_learners_competency_plus_bonus))\n",
    "        \n",
    "        # Plot the means\n",
    "        plt.plot(range(task_count), mean_competency_values,\n",
    "                    color=color, label=f'{key} mean competency')\n",
    "        plt.plot(range(task_count), mean_task_values,\n",
    "                    '.', color=color, label=f'{key} mean task')\n",
    "        plt.plot(range(task_count), mean_competency_plus_bonus_values,\n",
    "                    '--', color=color, label=f'{key} mean competency + scaffolding bonus')\n",
    "\n",
    "    plt.ylim(0, 1)\n",
    "    plt.xlim(0, task_count-1)\n",
    "    plt.ylabel(\"Mean Competency\")\n",
    "    plt.xlabel(\"Step\")\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\n",
    "        f\"sql_task_adaptation_mean.png\", dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveData(simulationLog, fileName):\n",
    "    records = []\n",
    "    order_id = 0\n",
    "\n",
    "    # Iterate through each student \n",
    "    for studentId in range(len(simulationLog)):\n",
    "        for taskIndex in range(len(simulationLog[studentId][\"tasks\"])):\n",
    "            task_order_id = order_id  # Create one order_id per task\n",
    "            # Iterate through categories\n",
    "            for category_i, (category_name, category_elements) in enumerate(DQL_MODEL.items()):\n",
    "                # Iterate through elements in category\n",
    "                for element_i, element in enumerate(category_elements):\n",
    "                    # Get delta for this element\n",
    "                    delta = simulationLog[studentId][\"deltas\"][taskIndex][category_name][element_i]\n",
    "                    \n",
    "                    # Create record with zero-padded IDs\n",
    "                    # Calculate position: (category_i * elements_per_category) + element_i\n",
    "                    skill_position = (category_i * len(category_elements)) + element_i\n",
    "                    record = {\n",
    "                        'order_id': f'{task_order_id+1:08d}',\n",
    "                        'user_id': f'{studentId+1:06d}',\n",
    "                        'sequence_id': f'{studentId+1:06d}',\n",
    "                        'skill_id': skill_position+1,\n",
    "                        'correct': 1 if delta > 0 else 0\n",
    "                    }\n",
    "                    records.append(record)\n",
    "            order_id += 1  # Increment order_id only after all records for a task are created\n",
    "\n",
    "    # Create and save dataframe\n",
    "    df = pd.DataFrame(records)\n",
    "    df.to_csv(\"../\" + fileName+'.csv', index=False, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TASK_COUNT = 10\n",
    "# LEARNER_COUNT = 1000\n",
    "# SIM_PARAM_COMPLEXITY_CONVERGATION_FACTOR = 0.5\n",
    "# SIM_PARAM_SCAFFOLDING_BONUS_DISTRIBUTION = (0.1, 0.002, 0, 0.2)\n",
    "\n",
    "def createSimData(task_count, learner_count, regulation, bonusDistribution):\n",
    "    simulationLog = simulate_task_adaptation(task_count, learner_count, regulation, bonusDistribution)\n",
    "    plot_mean_simulation_log(simulationLog, regulation)\n",
    "    return simulationLog\n",
    "    \n",
    "    \n",
    "simVariations = [\n",
    "    (35, 1000, 0.5, (0.05, 0.002, 0, 0.2)),\n",
    "    (15, 1000, 0.5, (0.1, 0.002, 0, 0.2)),\n",
    "    (10, 1000, 0.1, (0.2, 0.002, 0.1, 0.3))\n",
    "]\n",
    "\n",
    "# Initialize datasets array with the correct length\n",
    "dataset = []\n",
    "for sim in simVariations:\n",
    "    data = createSimData(*sim)\n",
    "    for value in data:\n",
    "        dataset.append(value)\n",
    "        \n",
    "saveData(dataset, 'dataset')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sql_task_adaptation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
